{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e2cab21",
   "metadata": {},
   "source": [
    "# Prepare the Training and Testing Data Sets\n",
    "\n",
    "First, we must load the data sets using the load_data_from_df function from transformer.py and then we will construct the data loader using the construct_loader, also from transformer.py. This notebook assumes you are running the model from the main section of this github repository, however we do set the current working directory to the soltrannet directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b9dab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "os.chdir('soltrannet')\n",
    "\n",
    "from soltrannet.data_utils import load_data_from_df, construct_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e59e673",
   "metadata": {},
   "source": [
    "Setting the specified random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88f06d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfile = 'Training_Data/train_subset1_1000.csv' \n",
    "testfile = 'Testing_Data/test_subset1_1000.csv' \n",
    "\n",
    "torch.manual_seed(420)\n",
    "np.random.seed(420)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53175896",
   "metadata": {},
   "source": [
    "Loading the Training & Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75872b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "trainX, trainy = load_data_from_df(trainfile, add_dummy_node=True, one_hot_formal_charge=True)\n",
    "data_loader = construct_loader(trainX, trainy, batch_size)\n",
    "\n",
    "testX, testy = load_data_from_df(testfile, add_dummy_node=True, one_hot_formal_charge=True)\n",
    "testdata_loader = construct_loader(testX, testy, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeec5ce",
   "metadata": {},
   "source": [
    "# Prepare Model\n",
    "\n",
    "The model parameters used below are the default values used for the SolTranNet architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfeff7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import make_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6941f7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_atom = trainX[0][0].shape[1] # It depends on the used featurization. To match \n",
    "                                #SolTranNet architecture d_atom should equal 28\n",
    "\n",
    "model_params = {\n",
    "    'd_atom': d_atom,\n",
    "    'd_model': 8,\n",
    "    'N': 8,\n",
    "    'h': 2,\n",
    "    'N_dense': 1,\n",
    "    'lambda_attention': 0.33, \n",
    "    'leaky_relu_slope': 0.0, \n",
    "    'dense_output_nonlinearity': 'relu', \n",
    "    'dropout': 0.1,\n",
    "    'aggregation_type': 'mean'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b61a736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac51399f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphTransformer(\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (1): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (2): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (3): Linear(in_features=8, out_features=8, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
       "          )\n",
       "          (dropout): ModuleList(\n",
       "            (0): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (1): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (2): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (3): Linear(in_features=8, out_features=8, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
       "          )\n",
       "          (dropout): ModuleList(\n",
       "            (0): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (1): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (2): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (3): Linear(in_features=8, out_features=8, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
       "          )\n",
       "          (dropout): ModuleList(\n",
       "            (0): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (1): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (2): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (3): Linear(in_features=8, out_features=8, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
       "          )\n",
       "          (dropout): ModuleList(\n",
       "            (0): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (1): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (2): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (3): Linear(in_features=8, out_features=8, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
       "          )\n",
       "          (dropout): ModuleList(\n",
       "            (0): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (1): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (2): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (3): Linear(in_features=8, out_features=8, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
       "          )\n",
       "          (dropout): ModuleList(\n",
       "            (0): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (1): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (2): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (3): Linear(in_features=8, out_features=8, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
       "          )\n",
       "          (dropout): ModuleList(\n",
       "            (0): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (1): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (2): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (3): Linear(in_features=8, out_features=8, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
       "          )\n",
       "          (dropout): ModuleList(\n",
       "            (0): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm()\n",
       "  )\n",
       "  (src_embed): Embeddings(\n",
       "    (lut): Linear(in_features=28, out_features=8, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (generator): Generator(\n",
       "    (proj): Linear(in_features=8, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "460f0f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the loss function\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90d86f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting Optimizer\n",
    "\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=1e-4,momentum=0.9,weight_decay=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa06614",
   "metadata": {},
   "source": [
    "# Run Training/Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dcbac3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brett\\brett.ondich\\SolTranNet_test\\soltrannet\\data_utils.py:259: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:210.)\n",
      "  return [FloatTensor(features) for features in (adjacency_list, features_list, labels)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "Epoch: 0\n",
      "Training RMSE: 3.415679183746601\n",
      "Training R2: 0.013341350056293303\n",
      "----------------------------------\n",
      "Epoch: 1\n",
      "Training RMSE: 3.03908591082807\n",
      "Training R2: 0.00836737764127391\n",
      "----------------------------------\n",
      "Epoch: 2\n",
      "Training RMSE: 2.733333304425917\n",
      "Training R2: 0.0011816092076617016\n",
      "----------------------------------\n",
      "Epoch: 3\n",
      "Training RMSE: 2.5372603917640246\n",
      "Training R2: 0.004344696458977883\n",
      "----------------------------------\n",
      "Epoch: 4\n",
      "Training RMSE: 2.4169266163391314\n",
      "Training R2: 0.06742326016621747\n",
      "----------------------------------\n",
      "Epoch: 5\n",
      "Training RMSE: 2.3517834217042255\n",
      "Training R2: 0.06520078662950021\n",
      "----------------------------------\n",
      "Epoch: 6\n",
      "Training RMSE: 2.3058599319046267\n",
      "Training R2: 0.1069868615021578\n",
      "----------------------------------\n",
      "Epoch: 7\n",
      "Training RMSE: 2.2737914960187595\n",
      "Training R2: 0.13074032526970367\n",
      "----------------------------------\n",
      "Epoch: 8\n",
      "Training RMSE: 2.2345569283864144\n",
      "Training R2: 0.1618056291313299\n",
      "----------------------------------\n",
      "Epoch: 9\n",
      "Training RMSE: 2.1920012326073794\n",
      "Training R2: 0.1897279980313417\n",
      "----------------------------------\n",
      "Epoch: 10\n",
      "Training RMSE: 2.173708713326116\n",
      "Training R2: 0.19284874128804502\n",
      "----------------------------------\n",
      "Epoch: 11\n",
      "Training RMSE: 2.1564351561476274\n",
      "Training R2: 0.20162891238946126\n",
      "----------------------------------\n",
      "Epoch: 12\n",
      "Training RMSE: 2.1380623581538183\n",
      "Training R2: 0.21401740282749804\n",
      "----------------------------------\n",
      "Epoch: 13\n",
      "Training RMSE: 2.1139831616268463\n",
      "Training R2: 0.23393166701858367\n",
      "----------------------------------\n",
      "Epoch: 14\n",
      "Training RMSE: 2.100508859131272\n",
      "Training R2: 0.2354000289907964\n",
      "----------------------------------\n",
      "Epoch: 15\n",
      "Training RMSE: 2.0749615807825115\n",
      "Training R2: 0.2576552256162008\n",
      "----------------------------------\n",
      "Epoch: 16\n",
      "Training RMSE: 2.070178610826633\n",
      "Training R2: 0.2513403578602704\n",
      "----------------------------------\n",
      "Epoch: 17\n",
      "Training RMSE: 2.087410765302466\n",
      "Training R2: 0.24072950309390004\n",
      "----------------------------------\n",
      "Epoch: 18\n",
      "Training RMSE: 2.0534722876486393\n",
      "Training R2: 0.26477108857599707\n",
      "----------------------------------\n",
      "Epoch: 19\n",
      "Training RMSE: 2.0521095956767903\n",
      "Training R2: 0.26568473565752937\n",
      "----------------------------------\n",
      "Epoch: 20\n",
      "Training RMSE: 2.0092367367804043\n",
      "Training R2: 0.29627053507466605\n",
      "----------------------------------\n",
      "Epoch: 21\n",
      "Training RMSE: 2.0302558614382056\n",
      "Training R2: 0.27875190749314255\n",
      "----------------------------------\n",
      "Epoch: 22\n",
      "Training RMSE: 2.00977739983492\n",
      "Training R2: 0.2915241465267636\n",
      "----------------------------------\n",
      "Epoch: 23\n",
      "Training RMSE: 1.9995263221686994\n",
      "Training R2: 0.298930095211119\n",
      "----------------------------------\n",
      "Epoch: 24\n",
      "Training RMSE: 2.0011811613825854\n",
      "Training R2: 0.29970376105374114\n",
      "----------------------------------\n",
      "Epoch: 25\n",
      "Training RMSE: 1.990501019195546\n",
      "Training R2: 0.3056281870378175\n",
      "----------------------------------\n",
      "Epoch: 26\n",
      "Training RMSE: 1.9630352054311544\n",
      "Training R2: 0.3255216789234102\n",
      "----------------------------------\n",
      "Epoch: 27\n",
      "Training RMSE: 1.9720709436658959\n",
      "Training R2: 0.31561628817525805\n",
      "----------------------------------\n",
      "Epoch: 28\n",
      "Training RMSE: 1.9762890960192765\n",
      "Training R2: 0.3152880505572154\n",
      "----------------------------------\n",
      "Epoch: 29\n",
      "Training RMSE: 1.9412658444514759\n",
      "Training R2: 0.33965756767610106\n",
      "----------------------------------\n",
      "Epoch: 30\n",
      "Training RMSE: 1.9639027256410324\n",
      "Training R2: 0.3215790890270872\n",
      "----------------------------------\n",
      "Epoch: 31\n",
      "Training RMSE: 1.9493867173410078\n",
      "Training R2: 0.3336137133588418\n",
      "----------------------------------\n",
      "Epoch: 32\n",
      "Training RMSE: 1.9365757391537348\n",
      "Training R2: 0.34313700205616365\n",
      "----------------------------------\n",
      "Epoch: 33\n",
      "Training RMSE: 1.965261802566484\n",
      "Training R2: 0.319328003316544\n",
      "----------------------------------\n",
      "Epoch: 34\n",
      "Training RMSE: 1.9343680338613127\n",
      "Training R2: 0.3432429619053781\n",
      "----------------------------------\n",
      "Epoch: 35\n",
      "Training RMSE: 1.9212265044272754\n",
      "Training R2: 0.3508268760257088\n",
      "----------------------------------\n",
      "Epoch: 36\n",
      "Training RMSE: 1.95118177549982\n",
      "Training R2: 0.33133379955629133\n",
      "----------------------------------\n",
      "Epoch: 37\n",
      "Training RMSE: 1.9376651211317386\n",
      "Training R2: 0.33850452713610896\n",
      "----------------------------------\n",
      "Epoch: 38\n",
      "Training RMSE: 1.9098553806495295\n",
      "Training R2: 0.35972279836784316\n",
      "----------------------------------\n",
      "Epoch: 39\n",
      "Training RMSE: 1.917803204601276\n",
      "Training R2: 0.3527221358615716\n",
      "----------------------------------\n",
      "Epoch: 40\n",
      "Training RMSE: 1.8954840744212453\n",
      "Training R2: 0.36767599252320426\n",
      "----------------------------------\n",
      "Epoch: 41\n",
      "Training RMSE: 1.9046503426364259\n",
      "Training R2: 0.36236050925382807\n",
      "----------------------------------\n",
      "Epoch: 42\n",
      "Training RMSE: 1.8863997205373977\n",
      "Training R2: 0.3757246436931285\n",
      "----------------------------------\n",
      "Epoch: 43\n",
      "Training RMSE: 1.9119205827367058\n",
      "Training R2: 0.35679317563288354\n",
      "----------------------------------\n",
      "Epoch: 44\n",
      "Training RMSE: 1.8729251150701032\n",
      "Training R2: 0.38285753889954344\n",
      "----------------------------------\n",
      "Epoch: 45\n",
      "Training RMSE: 1.8858799243464004\n",
      "Training R2: 0.3749724766478383\n",
      "----------------------------------\n",
      "Epoch: 46\n",
      "Training RMSE: 1.9015752709815357\n",
      "Training R2: 0.36332253120212415\n",
      "----------------------------------\n",
      "Epoch: 47\n",
      "Training RMSE: 1.8591849939666332\n",
      "Training R2: 0.3924962849339093\n",
      "----------------------------------\n",
      "Epoch: 48\n",
      "Training RMSE: 1.86989123194488\n",
      "Training R2: 0.38440270137548876\n",
      "----------------------------------\n",
      "Epoch: 49\n",
      "Training RMSE: 1.8943306571719831\n",
      "Training R2: 0.3710748716600093\n",
      "----------------------------------\n",
      "Epoch: 50\n",
      "Training RMSE: 1.8550368256279233\n",
      "Training R2: 0.39503208222695113\n",
      "----------------------------------\n",
      "Epoch: 51\n",
      "Training RMSE: 1.8651115910029044\n",
      "Training R2: 0.38898609001836665\n",
      "----------------------------------\n",
      "Epoch: 52\n",
      "Training RMSE: 1.8636941719722127\n",
      "Training R2: 0.3882871281991193\n",
      "----------------------------------\n",
      "Epoch: 53\n",
      "Training RMSE: 1.863914387644127\n",
      "Training R2: 0.38761918065378714\n",
      "----------------------------------\n",
      "Epoch: 54\n",
      "Training RMSE: 1.8594590113944107\n",
      "Training R2: 0.39343146926644534\n",
      "----------------------------------\n",
      "Epoch: 55\n",
      "Training RMSE: 1.8366094926797008\n",
      "Training R2: 0.41147510379051205\n",
      "----------------------------------\n",
      "Epoch: 56\n",
      "Training RMSE: 1.8363956213636898\n",
      "Training R2: 0.40592891849142554\n",
      "----------------------------------\n",
      "Epoch: 57\n",
      "Training RMSE: 1.8240230074808168\n",
      "Training R2: 0.41591039892743425\n",
      "----------------------------------\n",
      "Epoch: 58\n",
      "Training RMSE: 1.8295658511407835\n",
      "Training R2: 0.411229427018248\n",
      "----------------------------------\n",
      "Epoch: 59\n",
      "Training RMSE: 1.8348129111921023\n",
      "Training R2: 0.4076299028979543\n",
      "----------------------------------\n",
      "Epoch: 60\n",
      "Training RMSE: 1.8091656003440273\n",
      "Training R2: 0.4266560755929203\n",
      "----------------------------------\n",
      "Epoch: 61\n",
      "Training RMSE: 1.8040609324601706\n",
      "Training R2: 0.42935531474169636\n",
      "----------------------------------\n",
      "Epoch: 62\n",
      "Training RMSE: 1.8204583007123674\n",
      "Training R2: 0.41696017846700945\n",
      "----------------------------------\n",
      "Epoch: 63\n",
      "Training RMSE: 1.8016326871583976\n",
      "Training R2: 0.4299660855374217\n",
      "----------------------------------\n",
      "Epoch: 64\n",
      "Training RMSE: 1.8000334510726235\n",
      "Training R2: 0.4315551150563818\n",
      "----------------------------------\n",
      "Epoch: 65\n",
      "Training RMSE: 1.783843575612509\n",
      "Training R2: 0.44392881519700345\n",
      "----------------------------------\n",
      "Epoch: 66\n",
      "Training RMSE: 1.7960875544321548\n",
      "Training R2: 0.4334475860548775\n",
      "----------------------------------\n",
      "Epoch: 67\n",
      "Training RMSE: 1.8010078734354689\n",
      "Training R2: 0.4302425792836614\n",
      "----------------------------------\n",
      "Epoch: 68\n",
      "Training RMSE: 1.7763726353532756\n",
      "Training R2: 0.4500335207366018\n",
      "----------------------------------\n",
      "Epoch: 69\n",
      "Training RMSE: 1.7817880566310584\n",
      "Training R2: 0.44591810509961904\n",
      "----------------------------------\n",
      "Epoch: 70\n",
      "Training RMSE: 1.7712432912406706\n",
      "Training R2: 0.4522896280588104\n",
      "----------------------------------\n",
      "Epoch: 71\n",
      "Training RMSE: 1.7758310593884923\n",
      "Training R2: 0.44867934984592783\n",
      "----------------------------------\n",
      "Epoch: 72\n",
      "Training RMSE: 1.7435457155786371\n",
      "Training R2: 0.4732385623833673\n",
      "----------------------------------\n",
      "Epoch: 73\n",
      "Training RMSE: 1.7572703296850385\n",
      "Training R2: 0.4614324290653087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "Epoch: 74\n",
      "Training RMSE: 1.7540733755968383\n",
      "Training R2: 0.4645372150626944\n",
      "----------------------------------\n",
      "Epoch: 75\n",
      "Training RMSE: 1.755717562840668\n",
      "Training R2: 0.4613963842991761\n",
      "----------------------------------\n",
      "Epoch: 76\n",
      "Training RMSE: 1.7641794779562618\n",
      "Training R2: 0.4605647081482907\n",
      "----------------------------------\n",
      "Epoch: 77\n",
      "Training RMSE: 1.745176546292241\n",
      "Training R2: 0.4671765849870097\n",
      "----------------------------------\n",
      "Epoch: 78\n",
      "Training RMSE: 1.7474497588086813\n",
      "Training R2: 0.4677159909588535\n",
      "----------------------------------\n",
      "Epoch: 79\n",
      "Training RMSE: 1.7327971382831668\n",
      "Training R2: 0.4766527900605835\n",
      "----------------------------------\n",
      "Epoch: 80\n",
      "Training RMSE: 1.7215498795920037\n",
      "Training R2: 0.4813616381107064\n",
      "----------------------------------\n",
      "Epoch: 81\n",
      "Training RMSE: 1.7432827384887337\n",
      "Training R2: 0.47053560891972335\n",
      "----------------------------------\n",
      "Epoch: 82\n",
      "Training RMSE: 1.7368639439663884\n",
      "Training R2: 0.47025253333390815\n",
      "----------------------------------\n",
      "Epoch: 83\n",
      "Training RMSE: 1.7342040760181732\n",
      "Training R2: 0.4746032594691835\n",
      "----------------------------------\n",
      "Epoch: 84\n",
      "Training RMSE: 1.7250145165000692\n",
      "Training R2: 0.4781572317797829\n",
      "----------------------------------\n",
      "Epoch: 85\n",
      "Training RMSE: 1.6960441611253776\n",
      "Training R2: 0.4997067649143537\n",
      "----------------------------------\n",
      "Epoch: 86\n",
      "Training RMSE: 1.6826899285012957\n",
      "Training R2: 0.5073837126160843\n",
      "----------------------------------\n",
      "Epoch: 87\n",
      "Training RMSE: 1.6988074068719783\n",
      "Training R2: 0.4969557772112861\n",
      "----------------------------------\n",
      "Epoch: 88\n",
      "Training RMSE: 1.6871708019569787\n",
      "Training R2: 0.5024606308554975\n",
      "----------------------------------\n",
      "Epoch: 89\n",
      "Training RMSE: 1.7128648019740698\n",
      "Training R2: 0.48453580379229433\n",
      "----------------------------------\n",
      "Epoch: 90\n",
      "Training RMSE: 1.6695826825104576\n",
      "Training R2: 0.5140125711440606\n",
      "----------------------------------\n",
      "Epoch: 91\n",
      "Training RMSE: 1.6718262635765226\n",
      "Training R2: 0.5114591879017721\n",
      "----------------------------------\n",
      "Epoch: 92\n",
      "Training RMSE: 1.6912464409272672\n",
      "Training R2: 0.498899174625342\n",
      "----------------------------------\n",
      "Epoch: 93\n",
      "Training RMSE: 1.697729833483252\n",
      "Training R2: 0.4969492363312278\n",
      "----------------------------------\n",
      "Epoch: 94\n",
      "Training RMSE: 1.6773156923021908\n",
      "Training R2: 0.5054468448464743\n",
      "----------------------------------\n",
      "Epoch: 95\n",
      "Training RMSE: 1.6561444982014013\n",
      "Training R2: 0.5263746003332828\n",
      "----------------------------------\n",
      "Epoch: 96\n",
      "Training RMSE: 1.696248447758726\n",
      "Training R2: 0.4952172965345759\n",
      "----------------------------------\n",
      "Epoch: 97\n",
      "Training RMSE: 1.6772390543543982\n",
      "Training R2: 0.5078546209451257\n",
      "----------------------------------\n",
      "Epoch: 98\n",
      "Training RMSE: 1.6724885949118216\n",
      "Training R2: 0.5098759080383579\n",
      "----------------------------------\n",
      "Epoch: 99\n",
      "Training RMSE: 1.6483480154074006\n",
      "Training R2: 0.527158729860357\n",
      "----------------------------------\n",
      "Epoch: 100\n",
      "Training RMSE: 1.6686893054011172\n",
      "Training R2: 0.5107657435491919\n",
      "----------------------------------\n",
      "Epoch: 101\n",
      "Training RMSE: 1.6645735706543912\n",
      "Training R2: 0.5151782317252043\n",
      "----------------------------------\n",
      "Epoch: 102\n",
      "Training RMSE: 1.6841627801142327\n",
      "Training R2: 0.5017739376637949\n",
      "----------------------------------\n",
      "Epoch: 103\n",
      "Training RMSE: 1.656221939031943\n",
      "Training R2: 0.521757362030889\n",
      "----------------------------------\n",
      "Epoch: 104\n",
      "Training RMSE: 1.6605333980817039\n",
      "Training R2: 0.5173055821120991\n",
      "----------------------------------\n",
      "Epoch: 105\n",
      "Training RMSE: 1.6166408759887012\n",
      "Training R2: 0.5445962846037562\n",
      "----------------------------------\n",
      "Epoch: 106\n",
      "Training RMSE: 1.6318523200752217\n",
      "Training R2: 0.5339508522964288\n",
      "----------------------------------\n",
      "Epoch: 107\n",
      "Training RMSE: 1.6654434622112222\n",
      "Training R2: 0.5136701132953572\n",
      "----------------------------------\n",
      "Epoch: 108\n",
      "Training RMSE: 1.6855395046673092\n",
      "Training R2: 0.5004982503270026\n",
      "----------------------------------\n",
      "Epoch: 109\n",
      "Training RMSE: 1.669020483162103\n",
      "Training R2: 0.5119128537893801\n",
      "----------------------------------\n",
      "Epoch: 110\n",
      "Training RMSE: 1.6398174169634878\n",
      "Training R2: 0.5297949010726748\n",
      "----------------------------------\n",
      "Epoch: 111\n",
      "Training RMSE: 1.6510603782548698\n",
      "Training R2: 0.5215679817460072\n",
      "----------------------------------\n",
      "Epoch: 112\n",
      "Training RMSE: 1.6530301575281774\n",
      "Training R2: 0.520848841368952\n",
      "----------------------------------\n",
      "Epoch: 113\n",
      "Training RMSE: 1.6434441297858597\n",
      "Training R2: 0.5256982245734753\n",
      "----------------------------------\n",
      "Epoch: 114\n",
      "Training RMSE: 1.6286556739082827\n",
      "Training R2: 0.5381672443630774\n",
      "----------------------------------\n",
      "Epoch: 115\n",
      "Training RMSE: 1.6362131801355972\n",
      "Training R2: 0.5292967572769592\n",
      "----------------------------------\n",
      "Epoch: 116\n",
      "Training RMSE: 1.6426308806969765\n",
      "Training R2: 0.5254407921742117\n",
      "----------------------------------\n",
      "Epoch: 117\n",
      "Training RMSE: 1.6117258701521162\n",
      "Training R2: 0.5444838114143719\n",
      "----------------------------------\n",
      "Epoch: 118\n",
      "Training RMSE: 1.628039545336456\n",
      "Training R2: 0.537194430832083\n",
      "----------------------------------\n",
      "Epoch: 119\n",
      "Training RMSE: 1.6598760155764944\n",
      "Training R2: 0.5149121100075563\n",
      "----------------------------------\n",
      "Epoch: 120\n",
      "Training RMSE: 1.6243799927111187\n",
      "Training R2: 0.5390045926205614\n",
      "----------------------------------\n",
      "Epoch: 121\n",
      "Training RMSE: 1.6312078773085639\n",
      "Training R2: 0.5318656568987544\n",
      "----------------------------------\n",
      "Epoch: 122\n",
      "Training RMSE: 1.6166877494092688\n",
      "Training R2: 0.5401433326869329\n",
      "----------------------------------\n",
      "Epoch: 123\n",
      "Training RMSE: 1.600675622338192\n",
      "Training R2: 0.5526085151625346\n",
      "----------------------------------\n",
      "Epoch: 124\n",
      "Training RMSE: 1.6071801824573928\n",
      "Training R2: 0.5466095915336519\n",
      "----------------------------------\n",
      "Epoch: 125\n",
      "Training RMSE: 1.6093942905534318\n",
      "Training R2: 0.5458306751664755\n",
      "----------------------------------\n",
      "Epoch: 126\n",
      "Training RMSE: 1.6083029697236186\n",
      "Training R2: 0.5487205475123401\n",
      "----------------------------------\n",
      "Epoch: 127\n",
      "Training RMSE: 1.5790705489662735\n",
      "Training R2: 0.5619964931287998\n",
      "----------------------------------\n",
      "Epoch: 128\n",
      "Training RMSE: 1.6071028154986369\n",
      "Training R2: 0.5464468897953256\n",
      "----------------------------------\n",
      "Epoch: 129\n",
      "Training RMSE: 1.6083691743304105\n",
      "Training R2: 0.5443832447695632\n",
      "----------------------------------\n",
      "Epoch: 130\n",
      "Training RMSE: 1.6246491438891522\n",
      "Training R2: 0.5368108065134389\n",
      "----------------------------------\n",
      "Epoch: 131\n",
      "Training RMSE: 1.602760875305835\n",
      "Training R2: 0.549165892373701\n",
      "----------------------------------\n",
      "Epoch: 132\n",
      "Training RMSE: 1.592177444480458\n",
      "Training R2: 0.5545842447160387\n"
     ]
    }
   ],
   "source": [
    "iteration = 0\n",
    "for epoch in range(250):\n",
    "    epoch_preds=np.array([])\n",
    "    epoch_gold=np.array([])\n",
    "    for batch in data_loader:\n",
    "        iteration+=1\n",
    "        optimizer.zero_grad()\n",
    "        adjacency_matrix, node_features, y = batch\n",
    "        batch_mask = torch.sum(torch.abs(node_features), dim=-1) !=0\n",
    "        y_pred = model(node_features, batch_mask, adjacency_matrix, None)\n",
    "        \n",
    "        #accumulate the epoch training datas\n",
    "        epoch_gold=np.append(epoch_gold,y.tolist())\n",
    "        epoch_preds=np.append(epoch_preds,y_pred.tolist())\n",
    "        \n",
    "        loss=criterion(y_pred,y)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        #implementing gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),2)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if iteration%100==0:\n",
    "            #we evaluate the test set\n",
    "            model.eval()\n",
    "            gold=np.array([])\n",
    "            preds=np.array([])\n",
    "            \n",
    "            for t_batch in testdata_loader:\n",
    "                t_adjacency_matrix, t_node_features, t_y = t_batch\n",
    "                gold=np.append(gold,t_y.tolist())\n",
    "                t_batch_mask = torch.sum(torch.abs(t_node_features), dim=-1) !=0\n",
    "                t_y_pred = model(t_node_features, t_batch_mask, t_adjacency_matrix, None)\n",
    "                preds=np.append(preds,t_y_pred.tolist())\n",
    "            \n",
    "            test_rmse=np.sqrt(np.mean((preds-gold)**2))\n",
    "            test_r2=np.corrcoef(preds,gold)[0][1]**2\n",
    "            model.train()\n",
    "\n",
    "    #end of 1 epoch -- time to log the stats\n",
    "    \n",
    "    #print(epoch_preds)\n",
    "    #print(epoch_gold)\n",
    "    train_rmse = (np.sqrt(np.mean((epoch_preds-epoch_gold)**2))) \n",
    "    train_r2 = np.corrcoef(epoch_preds,epoch_gold)[0][1]**2\n",
    "    \n",
    "    print(f'----------------------------------')\n",
    "    print(f'Epoch: {epoch}')\n",
    "    print(f'Training RMSE: {train_rmse}')\n",
    "    print(f'Training R2: {train_r2}')\n",
    "    #print(f'Test RMSE: {test_rmse}')\n",
    "    #print(f'Test R2: {test_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329b09e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'trained.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f29d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "gold=np.array([])\n",
    "preds=np.array([])\n",
    "t0=time.time()\n",
    "train_times=[]\n",
    "\n",
    "for batch in data_loader:\n",
    "    t1=time.time()\n",
    "    adjacency_matrix, node_features, y = batch\n",
    "    tload=time.time()-t1\n",
    "    gold=np.append(gold,y)\n",
    "    batch_mask = torch.sum(torch.abs(node_features), dim=-1) != 0\n",
    "    y_pred = model(node_features, batch_mask, adjacency_matrix, None)\n",
    "    tpred=time.time()-t1\n",
    "    preds=np.append(preds,y_pred.tolist())\n",
    "    \n",
    "    train_times.append((tload,tpred))\n",
    "    \n",
    "ttime=time.time()-t0\n",
    "print('Overall Time: ',ttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fb3709",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2=np.corrcoef(preds,gold)[0][1]**2\n",
    "rmse=np.sqrt(np.mean((preds-gold)**2))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(gold,preds)\n",
    "z = np.polyfit(gold, preds, 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(gold, p(gold), color='r')\n",
    "plt.xlim([-10,2])\n",
    "plt.ylim([-10,2])\n",
    "plt.ylabel('Predictions')\n",
    "plt.xlabel('Actual')\n",
    "plt.title('%s without Quantum Data'%trainfile[14:])\n",
    "plt.text(-8, -8, r'RMSE=%s'%rmse)\n",
    "plt.text(-8, -9, r'R^2=%s'%r2)\n",
    "plt.savefig('Without_QData_%s.png'%trainfile[14:])\n",
    "plt.show()\n",
    "\n",
    "goldavg = np.mean(gold)\n",
    "print(goldavg)\n",
    "predavg = np.mean(preds)\n",
    "print(predavg)\n",
    "print('Train RMSE:',rmse)\n",
    "print('Train R2 :',r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831966f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the test_set\n",
    "\n",
    "gold=np.array([])\n",
    "preds=np.array([])\n",
    "t0=time.time()\n",
    "test_times=[]\n",
    "for batch in testdata_loader:\n",
    "    t1=time.time()\n",
    "    adjacency_matrix, node_features, y = batch\n",
    "    tload=time.time()-t1\n",
    "    gold=np.append(gold,y.tolist())\n",
    "    batch_mask = torch.sum(torch.abs(node_features), dim=-1) != 0\n",
    "    y_pred = model(node_features, batch_mask, adjacency_matrix, None)\n",
    "    tpred=time.time()-t1\n",
    "    preds=np.append(preds,y_pred.tolist())\n",
    "\n",
    "\n",
    "    test_times.append((tload,tpred))\n",
    "\n",
    "ttime=time.time()-t0\n",
    "print('Overall Time: ',ttime)\n",
    "\n",
    "r2=np.corrcoef(preds,gold)[0][1]**2\n",
    "rmse=np.sqrt(np.mean((preds-gold)**2))\n",
    "\n",
    "plt.scatter(gold,preds)\n",
    "z = np.polyfit(gold, preds, 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(gold, p(gold), color='r')\n",
    "plt.xlim([-10,2])\n",
    "plt.ylim([-10,2])\n",
    "plt.ylabel('Predictions')\n",
    "plt.xlabel('Actual')\n",
    "plt.title('%s without Quantum Data'%testfile[13:])\n",
    "plt.text(-8, -8, r'RMSE=%s'%rmse)\n",
    "plt.text(-8, -9, r'R^2=%s'%r2)\n",
    "plt.savefig('Without_QData_%s.png'%testfile[13:])\n",
    "plt.show()\n",
    "average = np.mean(gold)\n",
    "\n",
    "print('Test RMSE:',rmse)\n",
    "print('Test R2  :',r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1e70db",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "Try using Captum to see what is happening in the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
